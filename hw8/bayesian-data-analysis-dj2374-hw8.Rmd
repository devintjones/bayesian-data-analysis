---
title: "Bayesian Data Analysis HW8"
author: "Devin Jones dj2374"
date: "November 6, 2015"
output: pdf_document
---

## Estimating the Effects of Survey Incentives

Linear regression are typically used to generate insight from relationships in data to aid decision making. In this approach, researchers tend to rely heavily on the notion of statistical significance when modelling effects of parameters. This approach could be critiqued as flawed; by throwing away a parameter from a model, the variance that the variable contributed to the system is no longer captured. This analysis attempts to capture the effect of all available parameters in the data by using hierarchical Bayesian methods. 

The data consists of a collection of survey response rates, as well as conditions under which the survey was given, and whether or not there was an incentive. The conditions detailed in the data are the dollar value of the incentive, whether the incentive was provided before or after the survey, the form of the survey, ie gift or cash, the mode, ie face-to-face or over the phone, and the burden which can be thought of as the amount of effort required to complete the survey. 
There are 101 rows of data representing 39 surveys. Each row represents a survey administered under different conditions. 


## The Model

As a first pass at the model, we model the response rate of a survey, $\pi_i$ as the parameter in a binomial distribution given the number of people contacted, $N_i$

$$n_{i} \sim Binom(N_i, \pi_i)$$

From here we can consider $\pi_i$ as a function of the parameters in the model, $\beta_i$, as well as capture within survey variance with $\alpha$, and the total variance $\sigma^2$

$$\pi_i \sim Normal((X \beta_i) + \alpha_{j_i}, \sigma^2)$$

Here $X \beta_i$ is the linear predictor for condition $i$, and $\alpha_{j}$ is the random effect of one of the 39 surveys. 

Additionally we can place a prior distriubtion on the random effect $\alpha$

$$\alpha_j \sim Normal(0,\tau)$$

The above equations can be combined to form a posterior distribution. For large values of $N$, which we have in the data, we can use the normal approximation to the binomial

$$y_i \sim N(\pi_i,V_i)$$

where $V_i = y_i(1-y_i)/N_i$ and $y_i$ is the observed response rate. The normal approximation combined with above yields the following posterior distribution

$$y_i \sim N((X \beta)_i + \alpha_{j_i}, \sigma^2 + V_i)$$

This model can be framed easily in Stan. 

## Fitting the Model

The model was fit by computing the X matrix and passing the data to the Stan model below:

```{r,eval=F}
data{
  int N;
  int M;
  matrix[N,M] X;
  real<lower=0,upper=1> r[N];
  int idx[N]; // survey index
  int sid[N]; // number requested to survey
}
transformed data{
  real<lower=0> V[N];
  for(i in 1:N)
    V[i] <- (r[i] * (1 - r[i] ))/sid[i];
}
parameters{
  vector[M] betas;
  real<lower=0> sigma;
  real<lower=0> tau;
  vector[39] alpha;
}
model{
  alpha ~ normal(0,tau);

  for(i in 1:N)
    r[i] ~ normal(X[i] * betas + alpha[idx[i]],
      sigma + V[i] );
}
```

\pagebreak


First, a simple model was fit using no interaction terms. Then, first order interaction terms were included, and finally all interaction terms were included in the model. We can see that the effect of the incentive varies heavily as other terms are added to the model to account for additional variance in the data. The posterior means and 50% halfwidths are shown in the table below. 

Similar to the findings in the paper, the half width of the 50% posterior interval is larger than the mean estimate in some cases. This implies that there is very little signal in the data at this fine granularity. 

From the analysis, we can see that Mode and the interaction of incentive and mode have the biggest impact on the response rate. Offering money as an incentive increases the response rate by approximately 30 percentage points. 

Finally, we can see that the overall variance of the model $\sigma$ decreases as we add more explanatory variables to the model. This confirms the hypothesis that the interaction terms are indeed capturing some signal in the data. 

```{r, echo=FALSE}
setwd("~/Desktop/school/bayesian-data-analysis/hw8")
library(knitr)
data <- readRDS("final.rds")
row.names(data) <- NULL
names(data) <- c("Parameter","Trial 1", "Trial 2", "Trial 3")
kable(data)
```


\pagebreak

### Appendix: Code

```{r,eval=F}
setwd("~/Desktop/school/bayesian-data-analysis/hw8")
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

survey.resp <- read.table("http://www.stat.columbia.edu/~gelman/bda.course/incentives_data_clean.txt", skip = 12)

# indicator for incentive occurance
survey.resp$incentive <- ifelse(survey.resp$f==0,0,1)

idx <- data.frame(idx=1:39,sid=unique(survey.resp$sid))
survey.resp <- merge(survey.resp,idx)
survey.resp$one <- 1
survey.resp$r.dif <- NULL
survey.resp$v.dif <- NULL

stanData <- as.list(survey.resp)
stanData$X <- survey.resp[,c('one','incentive','m','b')]
stanData$N <- nrow(survey.resp)
stanData$M <- ncol(stanData$X)

#fit1 <- stan(file='model1.stan',data=stanData,iter=100)
#saveRDS(fit1,'fit1.rds')
fit1result <- stan(fit=fit1,data=stanData,iter=1000)


getInference <- function(stanData){
  fit1result <- stan(fit=fit1,data=stanData,iter=1000)
  results <- extract(fit1result)
  params <- cbind(results$betas,results$sigma,results$tau)
  colnames(params) <- c(names(stanData$X),'sigma','tau')
  
  means <- colMeans(params)
  
  intervals   <- do.call("rbind",(lapply(1:ncol(params),function(x) quantile(params[,x],c(.25,.75)))))
  halfWidth   <- (intervals[,2] - intervals[,1])/2
  return(100*data.frame(param=means,se=halfWidth))
}

trial1 <- getInference(stanData)


interaction1 <- stanData$X
interaction1 <- within(interaction1,{
  mb <- m*b
  incV <- incentive*survey.resp$v
  incT <- incentive*survey.resp$t
  incF <- incentive*survey.resp$f
  incM <- incentive*m
  incB <- incentive*b
})
  
stanData2 <- stanData
stanData2$X <- interaction1
stanData2$M <- ncol(interaction1)
trial2 <- getInference(stanData2)  


interaction2 <- within(interaction1,{
  incVT <- incV*survey.resp$t
  incVB <- incV*b
  incTB <- incT*b
  incVF <- incV*survey.resp$v
  incVM <- incV*m
  incTF <- incT*survey.resp$f
  incTM <- incT*m
  incFM <- incF*m
  incFB <- incF*b
  incMB <- incM*b
})

stanData3 <- stanData2
stanData3$X <- interaction2
stanData3$M <- ncol(interaction2)

trial3 <- getInference(stanData3)


clean3 <- data.frame(parameter=row.names(trial3),trial3=with(trial3, paste0(round(param,1)," (",round(se,1),")")),stringsAsFactors = F)
clean2 <- data.frame(parameter=row.names(trial2),trial2=with(trial2, paste0(round(param,1)," (",round(se,1),")")),stringsAsFactors = F)
clean1 <- data.frame(parameter=row.names(trial1),trial1=with(trial1, paste0(round(param,1)," (",round(se,1),")")),stringsAsFactors = F)
merged <- merge(clean3,merge(clean1,clean2,by="parameter",all.y=T),by="parameter",all.x=T)
merged[is.na(merged)]<-''

merged$Parameter <- c("Burden","Incentive X Burden","Incentive","Incentive X Form","Incentive X Form X Burden","Incentive X Form X Mode",
  "Incentive X Mode","Incetive X Mode X Burden","Incetive X Time","Incetive X Time X Burden","Incentive X Time X Form",
  "Incentive X Time X Mode","Incentive X Value","Incentive X Value X Burden","Incentive X Value X Form",
  "Incentive X Value X Mode","Incentive X Value X Time","Mode","Mode X Burden","Constant","Sigma","Tau")

ordered <- merged[c(20,3,18,1,19,13,4,7,9,2,5,6,8,10,11,12,14,15,16,17,21,22),]
final <- ordered[,c("Parameter","trial1","trial2","trial3")]
saveRDS(final,"final.rds")


```